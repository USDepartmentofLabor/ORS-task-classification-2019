{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Benchmarking\n",
    "\n",
    "The goal of the notebook is to benchmark and explore different algorithms and models to classify O\\*Net Task data into General Work Activities (GWA). The most promising algorithms will them be tuned and used for a similar task on Occupational Requirements Survey data. \n",
    "\n",
    "**Last Updated**: Wednesday July 24, 2019\n",
    "<br> **Author**: Rebecca Hu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>GWA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review and analyze legislation, laws, or publi...</td>\n",
       "      <td>Analyzing Data or Information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review and analyze legislation, laws, or publi...</td>\n",
       "      <td>Provide Consultation and Advice to Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Direct or coordinate an organization's financi...</td>\n",
       "      <td>Guiding, Directing, and Motivating Subordinates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Confer with board members, organization offici...</td>\n",
       "      <td>Communicating with Supervisors, Peers, or Subo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analyze operations to evaluate performance of ...</td>\n",
       "      <td>Analyzing Data or Information</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Task  \\\n",
       "0  Review and analyze legislation, laws, or publi...   \n",
       "1  Review and analyze legislation, laws, or publi...   \n",
       "2  Direct or coordinate an organization's financi...   \n",
       "3  Confer with board members, organization offici...   \n",
       "4  Analyze operations to evaluate performance of ...   \n",
       "\n",
       "                                                 GWA  \n",
       "0                      Analyzing Data or Information  \n",
       "1          Provide Consultation and Advice to Others  \n",
       "2    Guiding, Directing, and Motivating Subordinates  \n",
       "3  Communicating with Supervisors, Peers, or Subo...  \n",
       "4                      Analyzing Data or Information  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in O*Net data\n",
    "onet = pd.read_csv('onet_tasks_gwas.csv')\n",
    "onet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(onet['Task'], onet['GWA'], test_size=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "analyzer = TfidfVectorizer(stop_words = 'english').build_analyzer() #TfidfVectorizer(stop_words = None).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.645\n",
      "**********************************************************\n",
      "Test Accuracy:  0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "logit_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=stemmed_words)),\n",
    "    ('logit', LogisticRegression())])\n",
    "\n",
    "logit_pipe.fit(X_train, y_train)\n",
    "\n",
    "train_predicted = logit_pipe.predict(X_train)\n",
    "test_predicted = logit_pipe.predict(X_test)\n",
    "\n",
    "print('Training Accuracy: ', round(sum(train_predicted == y_train)/len(y_train), 3))\n",
    "train_p, train_r, train_f1, train_s = precision_recall_fscore_support(y_train, train_predicted, labels = y_train.unique())\n",
    "print('**********************************************************')\n",
    "print('Test Accuracy: ', round(sum(test_predicted == y_test)/len(y_test), 3))\n",
    "test_p, test_r, test_f1, test_s = precision_recall_fscore_support(y_test, test_predicted, labels = y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Decisions and Solving Problems\n",
      "******************************\n",
      "Precision:  0.6071428571428571\n",
      "Recall:  0.5862068965517241\n",
      "F1 Score:  0.5964912280701754\n",
      "Support:  58\n",
      "                                \n",
      "Documenting/Recording Information\n",
      "******************************\n",
      "Precision:  0.65625\n",
      "Recall:  0.6268656716417911\n",
      "F1 Score:  0.6412213740458016\n",
      "Support:  67\n",
      "                                \n",
      "Monitoring and Controlling Resources\n",
      "******************************\n",
      "Precision:  0.5833333333333334\n",
      "Recall:  0.25\n",
      "F1 Score:  0.35000000000000003\n",
      "Support:  28\n",
      "                                \n",
      "Provide Consultation and Advice to Others\n",
      "******************************\n",
      "Precision:  0.4748743718592965\n",
      "Recall:  0.78099173553719\n",
      "F1 Score:  0.590625\n",
      "Support:  242\n",
      "                                \n",
      "Interpreting the Meaning of Information for Others\n",
      "******************************\n",
      "Precision:  0.325\n",
      "Recall:  0.24528301886792453\n",
      "F1 Score:  0.27956989247311825\n",
      "Support:  53\n",
      "                                \n",
      "Handling and Moving Objects\n",
      "******************************\n",
      "Precision:  0.5769230769230769\n",
      "Recall:  0.5714285714285714\n",
      "F1 Score:  0.5741626794258372\n",
      "Support:  105\n",
      "                                \n",
      "Thinking Creatively\n",
      "******************************\n",
      "Precision:  0.5396825396825397\n",
      "Recall:  0.49635036496350365\n",
      "F1 Score:  0.5171102661596958\n",
      "Support:  137\n",
      "                                \n",
      "Performing General Physical Activities\n",
      "******************************\n",
      "Precision:  0.5\n",
      "Recall:  0.313953488372093\n",
      "F1 Score:  0.38571428571428573\n",
      "Support:  86\n",
      "                                \n",
      "Interacting With Computers\n",
      "******************************\n",
      "Precision:  0.5416666666666666\n",
      "Recall:  0.3333333333333333\n",
      "F1 Score:  0.4126984126984126\n",
      "Support:  39\n",
      "                                \n",
      "Controlling Machines and Processes\n",
      "******************************\n",
      "Precision:  0.4634146341463415\n",
      "Recall:  0.40425531914893614\n",
      "F1 Score:  0.4318181818181818\n",
      "Support:  47\n",
      "                                \n",
      "Monitor Processes, Materials, or Surroundings\n",
      "******************************\n",
      "Precision:  0.3974358974358974\n",
      "Recall:  0.5636363636363636\n",
      "F1 Score:  0.46616541353383456\n",
      "Support:  110\n",
      "                                \n",
      "Developing Objectives and Strategies\n",
      "******************************\n",
      "Precision:  0.5704225352112676\n",
      "Recall:  0.627906976744186\n",
      "F1 Score:  0.5977859778597786\n",
      "Support:  129\n",
      "                                \n",
      "Assisting and Caring for Others\n",
      "******************************\n",
      "Precision:  0.5955056179775281\n",
      "Recall:  0.75177304964539\n",
      "F1 Score:  0.6645768025078371\n",
      "Support:  141\n",
      "                                \n",
      "Communicating with Supervisors, Peers, or Subordinates\n",
      "******************************\n",
      "Precision:  0.5370370370370371\n",
      "Recall:  0.58\n",
      "F1 Score:  0.5576923076923077\n",
      "Support:  100\n",
      "                                \n",
      "Guiding, Directing, and Motivating Subordinates\n",
      "******************************\n",
      "Precision:  0.5737704918032787\n",
      "Recall:  0.6862745098039216\n",
      "F1 Score:  0.625\n",
      "Support:  51\n",
      "                                \n",
      "Getting Information\n",
      "******************************\n",
      "Precision:  0.5495495495495496\n",
      "Recall:  0.5083333333333333\n",
      "F1 Score:  0.528138528138528\n",
      "Support:  120\n",
      "                                \n",
      "Analyzing Data or Information\n",
      "******************************\n",
      "Precision:  0.3333333333333333\n",
      "Recall:  0.2\n",
      "F1 Score:  0.25\n",
      "Support:  10\n",
      "                                \n",
      "Communicating with Persons Outside Organization\n",
      "******************************\n",
      "Precision:  0.45454545454545453\n",
      "Recall:  0.4098360655737705\n",
      "F1 Score:  0.43103448275862066\n",
      "Support:  61\n",
      "                                \n",
      "Training and Teaching Others\n",
      "******************************\n",
      "Precision:  0.5\n",
      "Recall:  0.4931506849315068\n",
      "F1 Score:  0.496551724137931\n",
      "Support:  73\n",
      "                                \n",
      "Inspecting Equipment, Structures, or Material\n",
      "******************************\n",
      "Precision:  0.5\n",
      "Recall:  0.36363636363636365\n",
      "F1 Score:  0.4210526315789474\n",
      "Support:  22\n",
      "                                \n",
      "Operating Vehicles, Mechanized Devices, or Equipment\n",
      "******************************\n",
      "Precision:  0.4166666666666667\n",
      "Recall:  0.3125\n",
      "F1 Score:  0.35714285714285715\n",
      "Support:  32\n",
      "                                \n",
      "Coaching and Developing Others\n",
      "******************************\n",
      "Precision:  0.543859649122807\n",
      "Recall:  0.543859649122807\n",
      "F1 Score:  0.543859649122807\n",
      "Support:  57\n",
      "                                \n",
      "Judging the Qualities of Things, Services, or People\n",
      "******************************\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.36363636363636365\n",
      "F1 Score:  0.4705882352941177\n",
      "Support:  11\n",
      "                                \n",
      "Scheduling Work and Activities\n",
      "******************************\n",
      "Precision:  0.3333333333333333\n",
      "Recall:  0.25\n",
      "F1 Score:  0.28571428571428575\n",
      "Support:  16\n",
      "                                \n",
      "Performing Administrative Activities\n",
      "******************************\n",
      "Precision:  0.625\n",
      "Recall:  0.5263157894736842\n",
      "F1 Score:  0.5714285714285714\n",
      "Support:  19\n",
      "                                \n",
      "Estimating the Quantifiable Characteristics of Products, Events, or Information\n",
      "******************************\n",
      "Precision:  0.38461538461538464\n",
      "Recall:  0.19230769230769232\n",
      "F1 Score:  0.2564102564102564\n",
      "Support:  26\n",
      "                                \n",
      "Repairing and Maintaining Mechanical Equipment\n",
      "******************************\n",
      "Precision:  0.4782608695652174\n",
      "Recall:  0.2894736842105263\n",
      "F1 Score:  0.360655737704918\n",
      "Support:  38\n",
      "                                \n",
      "Organizing, Planning, and Prioritizing Work\n",
      "******************************\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "Support:  9\n",
      "                                \n",
      "Processing Information\n",
      "******************************\n",
      "Precision:  0.8333333333333334\n",
      "Recall:  0.2777777777777778\n",
      "F1 Score:  0.4166666666666667\n",
      "Support:  18\n",
      "                                \n",
      "Staffing Organizational Units\n",
      "******************************\n",
      "Precision:  0.5384615384615384\n",
      "Recall:  0.22580645161290322\n",
      "F1 Score:  0.3181818181818182\n",
      "Support:  31\n",
      "                                \n",
      "Establishing and Maintaining Interpersonal Relationships\n",
      "******************************\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.5263157894736842\n",
      "F1 Score:  0.5882352941176471\n",
      "Support:  19\n",
      "                                \n",
      "Updating and Using Relevant Knowledge\n",
      "******************************\n",
      "Precision:  1.0\n",
      "Recall:  0.07692307692307693\n",
      "F1 Score:  0.14285714285714288\n",
      "Support:  13\n",
      "                                \n",
      "Identifying Objects, Actions, and Events\n",
      "******************************\n",
      "Precision:  0.5714285714285714\n",
      "Recall:  0.19047619047619047\n",
      "F1 Score:  0.2857142857142857\n",
      "Support:  21\n",
      "                                \n",
      "Evaluating Information to Determine Compliance with Standards\n",
      "******************************\n",
      "Precision:  1.0\n",
      "Recall:  0.14285714285714285\n",
      "F1 Score:  0.25\n",
      "Support:  14\n",
      "                                \n",
      "Selling or Influencing Others\n",
      "******************************\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "Support:  11\n",
      "                                \n",
      "Performing for or Working Directly with the Public\n",
      "******************************\n",
      "Precision:  1.0\n",
      "Recall:  0.25\n",
      "F1 Score:  0.4\n",
      "Support:  4\n",
      "                                \n",
      "Resolving Conflicts and Negotiating with Others\n",
      "******************************\n",
      "Precision:  1.0\n",
      "Recall:  0.2\n",
      "F1 Score:  0.33333333333333337\n",
      "Support:  5\n",
      "                                \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_p)):\n",
    "    print(y_test.unique()[i])\n",
    "    print('******************************')\n",
    "    print('Precision: ', test_p[i])\n",
    "    print('Recall: ', test_r[i])\n",
    "    print('F1 Score: ', test_f1[i])\n",
    "    print('Support: ', test_s[i])\n",
    "    print('                                ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.55\n",
      "Average Recall:  0.383\n",
      "Average F1 Score:  0.416\n",
      "Average Support:  54.676\n"
     ]
    }
   ],
   "source": [
    "print('Average Precision: ', round(np.mean(test_p),3))\n",
    "print('Average Recall: ', round(np.mean(test_r), 3))\n",
    "print('Average F1 Score: ', round(np.mean(test_f1), 3))\n",
    "print('Average Support: ', round(np.mean(test_s), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sk-Learn's implementation of logistic regression can automatically detect multiple classes, but will not automatically assign multiple labels. A simple logistic regression works quite well considering we have 41 (really only 37) unique classes. We will consider this model our baseline. Next let's try multinomial naive bayes. \n",
    "\n",
    "---\n",
    "\n",
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.485\n",
      "**********************************************************\n",
      "Test Accuracy:  0.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=stemmed_words)),\n",
    "    ('multinom nb', MultinomialNB())])\n",
    "\n",
    "mnb_pipe.fit(X_train, y_train)\n",
    "\n",
    "train_predicted = mnb_pipe.predict(X_train)\n",
    "test_predicted = mnb_pipe.predict(X_test)\n",
    "\n",
    "print('Training Accuracy: ', round(sum(train_predicted == y_train)/len(y_train), 3))\n",
    "train_p, train_r, train_f1, train_s = precision_recall_fscore_support(y_train, train_predicted, labels = y_train.unique())\n",
    "print('**********************************************************')\n",
    "print('Test Accuracy: ', round(sum(test_predicted == y_test)/len(y_test), 3))\n",
    "test_p, test_r, test_f1, test_s = precision_recall_fscore_support(y_test, test_predicted, labels = y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.327\n",
      "Average Recall:  0.194\n",
      "Average F1 Score:  0.195\n",
      "Average Support:  54.676\n"
     ]
    }
   ],
   "source": [
    "print('Average Precision: ', round(np.mean(test_p),3))\n",
    "print('Average Recall: ', round(np.mean(test_r), 3))\n",
    "print('Average F1 Score: ', round(np.mean(test_f1), 3))\n",
    "print('Average Support: ', round(np.mean(test_s), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive bayes performed slightly worse than the baseline. Let's try a more complex model, a RandomForest model.\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accurcy:  0.548\n",
      "**********************************************************\n",
      "Test Accuracy:  0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=stemmed_words)),\n",
    "    ('extra trees', RandomForestClassifier(n_estimators = 1000, min_samples_leaf = 3, max_depth = 100))])\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "train_predicted = rf_pipe.predict(X_train)\n",
    "test_predicted = rf_pipe.predict(X_test)\n",
    "\n",
    "print('Training Accurcy: ', round(sum(train_predicted == y_train)/len(y_train), 3))\n",
    "train_p, train_r, train_f1, train_s = precision_recall_fscore_support(y_train, train_predicted, labels = y_train.unique())\n",
    "print('**********************************************************')\n",
    "print('Test Accuracy: ', round(sum(test_predicted == y_test)/len(y_test), 3))\n",
    "test_p, test_r, test_f1, test_s = precision_recall_fscore_support(y_test, test_predicted, labels = y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'task': X_test, 'actual': y_test, 'predicted':list(test_predicted)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.421\n",
      "Average Recall:\t   0.297\n",
      "Average F1 Score:  0.318\n",
      "Average Support:   54.676\n"
     ]
    }
   ],
   "source": [
    "print('Average Precision:', round(np.mean(test_p),3))\n",
    "print('Average Recall:\\t  ', round(np.mean(test_r), 3))\n",
    "print('Average F1 Score: ', round(np.mean(test_f1), 3))\n",
    "print('Average Support:  ', round(np.mean(test_s), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able manipulate the parameters of the model to improve the overall performane of the model, but it still doesn't perform as well as the baseline and takes longer to run. Next, we'll try a linear SVM.\n",
    "\n",
    "---\n",
    "\n",
    "### Support Vector Machine with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.653\n",
      "**********************************************************\n",
      "Test Accuracy:  0.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=stemmed_words)),\n",
    "    ('svc', SVC(C = 0.4, kernel = 'linear'))])\n",
    "\n",
    "svc_pipe.fit(X_train, y_train)\n",
    "\n",
    "train_predicted = svc_pipe.predict(X_train)\n",
    "test_predicted = svc_pipe.predict(X_test)\n",
    "\n",
    "print('Training Accuracy: ', round(sum(train_predicted == y_train)/len(y_train), 3))\n",
    "train_p, train_r, train_f1, train_s = precision_recall_fscore_support(y_train, train_predicted, labels = y_train.unique())\n",
    "print('**********************************************************')\n",
    "print('Test Accuracy: ', round(sum(test_predicted == y_test)/len(y_test), 3))\n",
    "test_p, test_r, test_f1, test_s = precision_recall_fscore_support(y_test, test_predicted, labels = y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.502\n",
      "Average Recall:  0.382\n",
      "Average F1 Score:  0.407\n",
      "Average Support:  54.676\n"
     ]
    }
   ],
   "source": [
    "print('Average Precision: ', round(np.mean(test_p),3))\n",
    "print('Average Recall: ', round(np.mean(test_r), 3))\n",
    "print('Average F1 Score: ', round(np.mean(test_f1), 3))\n",
    "print('Average Support: ', round(np.mean(test_s), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the parameters to the model were slightly altered, the results were quite similar to that of the logistic regression. The precision and recall scores for the model are not too far apart. This could be a promising model. Next, we'll try an emsemble method, gradient boosting.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.875\n",
      "**********************************************************\n",
      "Test Accuracy:  0.396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=stemmed_words)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators = 100, min_samples_leaf = 3, max_depth = 100))])\n",
    "\n",
    "gb_pipe.fit(X_train, y_train)\n",
    "\n",
    "train_predicted = gb_pipe.predict(X_train)\n",
    "test_predicted = gb_pipe.predict(X_test)\n",
    "\n",
    "print('Training Accuracy: ', round(sum(train_predicted == y_train)/len(y_train), 3))\n",
    "train_p, train_r, train_f1, train_s = precision_recall_fscore_support(y_train, train_predicted, labels = y_train.unique())\n",
    "print('**********************************************************')\n",
    "print('Test Accuracy: ', round(sum(test_predicted == y_test)/len(y_test), 3))\n",
    "test_p, test_r, test_f1, test_s = precision_recall_fscore_support(y_test, test_predicted, labels = y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.397\n",
      "Average Recall:  0.353\n",
      "Average F1 Score:  0.363\n",
      "Average Support:  54.676\n"
     ]
    }
   ],
   "source": [
    "print('Average Precision: ', round(np.mean(test_p),3))\n",
    "print('Average Recall: ', round(np.mean(test_r), 3))\n",
    "print('Average F1 Score: ', round(np.mean(test_f1), 3))\n",
    "print('Average Support: ', round(np.mean(test_s), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a similar problem with the gradient boosting, where the training accuracy is high but the test accuracy is quite low, also the computational resources required to run this mode are incredibly time intensive. Next we'll go back to a tree model. ExtraTrees differes from RandomForest in that they typically result in larger trees because of how splits are chosen. We'll try this algorithm to see if will be able to generalize better then the RandomForest. \n",
    "\n",
    "---\n",
    "\n",
    "### Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.496\n",
      "**********************************************************\n",
      "Test Accuracy:  0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "xtrees_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=stemmed_words)),\n",
    "    ('extra trees', ExtraTreesClassifier(n_estimators = 100, max_depth = 100, min_samples_leaf = 5))])\n",
    "\n",
    "xtrees_pipe.fit(X_train, y_train)\n",
    "\n",
    "train_predicted = xtrees_pipe.predict(X_train)\n",
    "test_predicted = xtrees_pipe.predict(X_test)\n",
    "\n",
    "print('Training Accuracy: ', round(sum(train_predicted == y_train)/len(y_train), 3))\n",
    "train_p, train_r, train_f1, train_s = precision_recall_fscore_support(y_train, train_predicted, labels = y_train.unique())\n",
    "print('**********************************************************')\n",
    "print('Test Accuracy: ', round(sum(test_predicted == y_test)/len(y_test), 3))\n",
    "test_p, test_r, test_f1, test_s = precision_recall_fscore_support(y_test, test_predicted, labels = y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Information\n",
      "******************************\n",
      "Precision:  0.44206008583690987\n",
      "Recall:  0.7862595419847328\n",
      "F1 Score:  0.5659340659340659\n",
      "Support:  131\n",
      "                                \n",
      "Updating and Using Relevant Knowledge\n",
      "******************************\n",
      "Precision:  0.49333333333333335\n",
      "Recall:  0.40217391304347827\n",
      "F1 Score:  0.4431137724550898\n",
      "Support:  92\n",
      "                                \n",
      "Performing General Physical Activities\n",
      "******************************\n",
      "Precision:  0.2857142857142857\n",
      "Recall:  0.07017543859649122\n",
      "F1 Score:  0.11267605633802817\n",
      "Support:  57\n",
      "                                \n",
      "Selling or Influencing Others\n",
      "******************************\n",
      "Precision:  0.39236641221374047\n",
      "Recall:  0.9178571428571428\n",
      "F1 Score:  0.5497326203208557\n",
      "Support:  280\n",
      "                                \n",
      "Training and Teaching Others\n",
      "******************************\n",
      "Precision:  0.6385542168674698\n",
      "Recall:  0.39849624060150374\n",
      "F1 Score:  0.4907407407407407\n",
      "Support:  133\n",
      "                                \n",
      "Estimating the Quantifiable Characteristics of Products, Events, or Information\n",
      "******************************\n",
      "Precision:  0.4444444444444444\n",
      "Recall:  0.15384615384615385\n",
      "F1 Score:  0.2285714285714286\n",
      "Support:  26\n",
      "                                \n",
      "Monitor Processes, Materials, or Surroundings\n",
      "******************************\n",
      "Precision:  1.0\n",
      "Recall:  0.06060606060606061\n",
      "F1 Score:  0.1142857142857143\n",
      "Support:  33\n",
      "                                \n",
      "Handling and Moving Objects\n",
      "******************************\n",
      "Precision:  1.0\n",
      "Recall:  0.1111111111111111\n",
      "F1 Score:  0.19999999999999998\n",
      "Support:  9\n",
      "                                \n",
      "Thinking Creatively\n",
      "******************************\n",
      "Precision:  0.5434782608695652\n",
      "Recall:  0.5617977528089888\n",
      "F1 Score:  0.5524861878453039\n",
      "Support:  89\n",
      "                                \n",
      "Performing for or Working Directly with the Public\n",
      "******************************\n",
      "Precision:  0.5\n",
      "Recall:  0.1\n",
      "F1 Score:  0.16666666666666669\n",
      "Support:  10\n",
      "                                \n",
      "Judging the Qualities of Things, Services, or People\n",
      "******************************\n",
      "Precision:  0.48936170212765956\n",
      "Recall:  0.6216216216216216\n",
      "F1 Score:  0.5476190476190476\n",
      "Support:  111\n",
      "                                \n",
      "Guiding, Directing, and Motivating Subordinates\n",
      "******************************\n",
      "Precision:  0.3875\n",
      "Recall:  0.543859649122807\n",
      "F1 Score:  0.45255474452554745\n",
      "Support:  114\n",
      "                                \n",
      "Controlling Machines and Processes\n",
      "******************************\n",
      "Precision:  0.5925925925925926\n",
      "Recall:  0.32\n",
      "F1 Score:  0.41558441558441556\n",
      "Support:  50\n",
      "                                \n",
      "Communicating with Supervisors, Peers, or Subordinates\n",
      "******************************\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "Support:  3\n",
      "                                \n",
      "Performing Administrative Activities\n",
      "******************************\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "Support:  30\n",
      "                                \n",
      "Assisting and Caring for Others\n",
      "******************************\n",
      "Precision:  0.4909090909090909\n",
      "Recall:  0.5684210526315789\n",
      "F1 Score:  0.5268292682926828\n",
      "Support:  95\n",
      "                                \n",
      "Interpreting the Meaning of Information for Others\n",
      "******************************\n",
      "Precision:  0.6\n",
      "Recall:  0.036585365853658534\n",
      "F1 Score:  0.06896551724137931\n",
      "Support:  82\n",
      "                                \n",
      "Making Decisions and Solving Problems\n",
      "******************************\n",
      "Precision:  0.4823529411764706\n",
      "Recall:  0.5616438356164384\n",
      "F1 Score:  0.5189873417721519\n",
      "Support:  73\n",
      "                                \n",
      "Repairing and Maintaining Mechanical Equipment\n",
      "******************************\n",
      "Precision:  0.5294117647058824\n",
      "Recall:  0.45\n",
      "F1 Score:  0.48648648648648646\n",
      "Support:  60\n",
      "                                \n",
      "Inspecting Equipment, Structures, or Material\n",
      "******************************\n",
      "Precision:  0.48148148148148145\n",
      "Recall:  0.21311475409836064\n",
      "F1 Score:  0.29545454545454547\n",
      "Support:  61\n",
      "                                \n",
      "Documenting/Recording Information\n",
      "******************************\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "Support:  8\n",
      "                                \n",
      "Analyzing Data or Information\n",
      "******************************\n",
      "Precision:  0.5714285714285714\n",
      "Recall:  0.1951219512195122\n",
      "F1 Score:  0.29090909090909095\n",
      "Support:  41\n",
      "                                \n",
      "Scheduling Work and Activities\n",
      "******************************\n",
      "Precision:  0.2857142857142857\n",
      "Recall:  0.13333333333333333\n",
      "F1 Score:  0.18181818181818182\n",
      "Support:  15\n",
      "                                \n",
      "Processing Information\n",
      "******************************\n",
      "Precision:  0.5\n",
      "Recall:  0.05263157894736842\n",
      "F1 Score:  0.09523809523809525\n",
      "Support:  19\n",
      "                                \n",
      "Provide Consultation and Advice to Others\n",
      "******************************\n",
      "Precision:  0.46153846153846156\n",
      "Recall:  0.17647058823529413\n",
      "F1 Score:  0.25531914893617025\n",
      "Support:  34\n",
      "                                \n",
      "Monitoring and Controlling Resources\n",
      "******************************\n",
      "Precision:  0.5714285714285714\n",
      "Recall:  0.12903225806451613\n",
      "F1 Score:  0.2105263157894737\n",
      "Support:  31\n",
      "                                \n",
      "Evaluating Information to Determine Compliance with Standards\n",
      "******************************\n",
      "Precision:  0.6\n",
      "Recall:  0.2727272727272727\n",
      "F1 Score:  0.37499999999999994\n",
      "Support:  11\n",
      "                                \n",
      "Interacting With Computers\n",
      "******************************\n",
      "Precision:  0.6666666666666666\n",
      "Recall:  0.19047619047619047\n",
      "F1 Score:  0.2962962962962963\n",
      "Support:  21\n",
      "                                \n",
      "Developing Objectives and Strategies\n",
      "******************************\n",
      "Precision:  0.6875\n",
      "Recall:  0.4852941176470588\n",
      "F1 Score:  0.5689655172413792\n",
      "Support:  68\n",
      "                                \n",
      "Operating Vehicles, Mechanized Devices, or Equipment\n",
      "******************************\n",
      "Precision:  0.47692307692307695\n",
      "Recall:  0.44285714285714284\n",
      "F1 Score:  0.45925925925925926\n",
      "Support:  70\n",
      "                                \n",
      "Resolving Conflicts and Negotiating with Others\n",
      "******************************\n",
      "Precision:  0.3333333333333333\n",
      "Recall:  0.023809523809523808\n",
      "F1 Score:  0.044444444444444446\n",
      "Support:  42\n",
      "                                \n",
      "Communicating with Persons Outside Organization\n",
      "******************************\n",
      "Precision:  0.5555555555555556\n",
      "Recall:  0.6363636363636364\n",
      "F1 Score:  0.5932203389830508\n",
      "Support:  55\n",
      "                                \n",
      "Staffing Organizational Units\n",
      "******************************\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "Support:  9\n",
      "                                \n",
      "Coaching and Developing Others\n",
      "******************************\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "Support:  6\n",
      "                                \n",
      "Organizing, Planning, and Prioritizing Work\n",
      "******************************\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1 Score:  0.0\n",
      "Support:  23\n",
      "                                \n",
      "Establishing and Maintaining Interpersonal Relationships\n",
      "******************************\n",
      "Precision:  0.25\n",
      "Recall:  0.07142857142857142\n",
      "F1 Score:  0.11111111111111112\n",
      "Support:  14\n",
      "                                \n",
      "Identifying Objects, Actions, and Events\n",
      "******************************\n",
      "Precision:  0.7857142857142857\n",
      "Recall:  0.6470588235294118\n",
      "F1 Score:  0.7096774193548386\n",
      "Support:  17\n",
      "                                \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_p)):\n",
    "    print(y_test.unique()[i])\n",
    "    print('******************************')\n",
    "    print('Precision: ', test_p[i])\n",
    "    print('Recall: ', test_r[i])\n",
    "    print('F1 Score: ', test_f1[i])\n",
    "    print('Support: ', test_s[i])\n",
    "    print('                                ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.447\n",
      "Average Recall:  0.279\n",
      "Average F1 Score:  0.295\n",
      "Average Support:  54.676\n"
     ]
    }
   ],
   "source": [
    "print('Average Precision: ', round(np.mean(test_p),3))\n",
    "print('Average Recall: ', round(np.mean(test_r), 3))\n",
    "print('Average F1 Score: ', round(np.mean(test_f1), 3))\n",
    "print('Average Support: ', round(np.mean(test_s), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can able to reduce overfitting but configuring the parameters, but the precision and recall of this algorithm are not well balanced, and it failed to perform as well as the baseline model.\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#‘lbfgs’, ‘sgd’, ‘adam’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.523\n",
      "**********************************************************\n",
      "Test Accuracy:  0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=stemmed_words)),\n",
    "    ('extra trees', MLPClassifier(activation = 'tanh', alpha = 0.5, learning_rate = 'adaptive'))])\n",
    "\n",
    "mlp_pipe.fit(X_train, y_train)\n",
    "\n",
    "train_predicted = mlp_pipe.predict(X_train)\n",
    "test_predicted = mlp_pipe.predict(X_test)\n",
    "\n",
    "print('Training Accuracy: ', round(sum(train_predicted == y_train)/len(y_train), 3))\n",
    "train_p, train_r, train_f1, train_s = precision_recall_fscore_support(y_train, train_predicted, labels = y_train.unique())\n",
    "print('**********************************************************')\n",
    "print('Test Accuracy: ', round(sum(test_predicted == y_test)/len(y_test), 3))\n",
    "test_p, test_r, test_f1, test_s = precision_recall_fscore_support(y_test, test_predicted, labels = y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.259\n",
      "Average Recall:  0.239\n",
      "Average F1 Score:  0.224\n",
      "Average Support:  54.676\n"
     ]
    }
   ],
   "source": [
    "print('Average Precision: ', round(np.mean(test_p),3))\n",
    "print('Average Recall: ', round(np.mean(test_r), 3))\n",
    "print('Average F1 Score: ', round(np.mean(test_f1), 3))\n",
    "print('Average Support: ', round(np.mean(test_s), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to configure the alpha parameter to decrease over-fitting, and the precision and recall scores are quite balanced, but the performance of the model is still worse than our baseline model. Next, we'll try another ensemble method known as AdaBoost. (I would try XGBoost, but I don't have it installed on this computer; I would recommend trying it though!)\n",
    "\n",
    "---\n",
    "\n",
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.35\n",
      "**********************************************************\n",
      "Test Accuracy:  0.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=stemmed_words)),\n",
    "    ('extra gb', AdaBoostClassifier(n_estimators = 1000, algorithm = 'SAMME', learning_rate = 2.5))])\n",
    "\n",
    "ada_pipe.fit(X_train, y_train)\n",
    "\n",
    "train_predicted = ada_pipe.predict(X_train)\n",
    "test_predicted = ada_pipe.predict(X_test)\n",
    "\n",
    "print('Training Accuracy: ', round(sum(train_predicted == y_train)/len(y_train), 3))\n",
    "train_p, train_r, train_f1, train_s = precision_recall_fscore_support(y_train, train_predicted, labels = y_train.unique())\n",
    "print('**********************************************************')\n",
    "print('Test Accuracy: ', round(sum(test_predicted == y_test)/len(y_test), 3))\n",
    "test_p, test_r, test_f1, test_s = precision_recall_fscore_support(y_test, test_predicted, labels = y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.352\n",
      "Average Recall:  0.08\n",
      "Average F1 Score:  0.083\n",
      "Average Support:  54.676\n"
     ]
    }
   ],
   "source": [
    "print('Average Precision: ', round(np.mean(test_p),3))\n",
    "print('Average Recall: ', round(np.mean(test_r), 3))\n",
    "print('Average F1 Score: ', round(np.mean(test_f1), 3))\n",
    "print('Average Support: ', round(np.mean(test_s), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to manipulate the parameters for reduce over-fitting but the AdaBoost algorithm does not perfom as well as the baseline. Also, the recall score is quite low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
